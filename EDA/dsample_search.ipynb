{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install --quiet --user --upgrade ipykernel\n",
    "!{sys.executable} -m pip install --quiet --user --upgrade pandas==1.*\n",
    "!{sys.executable} -m pip install --quiet --user --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from goodies import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dcollect import plugins\n",
    "\n",
    "modules = {'http': plugins.fasthttp()}\n",
    "headers = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YouTube (United States)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial setup. Be sure to have your API key ready. For details on how to obtain an API key, read [YouTube Data API Overview, Introduction: Before you start](https://developers.google.com/youtube/v3/getting-started#before-you-start)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouTube Data API Key: AIzaSyAa8yy0GdcGPHdtD083HiGGx_S0vMPScDM\n",
      "Is this an explorer key? [Y/N]: Y\n",
      "Dataset Name: random_extended_ascii\n",
      "Sample size per query: 200\n"
     ]
    }
   ],
   "source": [
    "from dcollect import api_youtube as youtube\n",
    "from dcollect import api_youtubei as youtubei\n",
    "\n",
    "# This key is for testing ONLY. DO NOT release to the public!\n",
    "api_experiment = False\n",
    "api_key_testing = None\n",
    "api_key = os.environ.get('YOUTUBE_API_KEY') or api_key_testing\n",
    "\n",
    "if not api_key:\n",
    "    api_key = os.environ.get('YOUTUBE_EXPLORER_API_KEY')\n",
    "    if api_key: \n",
    "        api_experiment = True\n",
    "    else: \n",
    "        api_key = input('YouTube Data API Key: ')\n",
    "        api_experiment = (input('Is this an explorer key? [Y/N]: ') == 'Y')\n",
    "\n",
    "dataset_id = os.environ.get('DATASET_NAME')\n",
    "if dataset_id == None:\n",
    "    dataset_id = input('Dataset Name: ')\n",
    "    \n",
    "sample_size_per_query_default = 1000000\n",
    "sample_size_per_query = os.environ.get('SAMPLE_SIZE_PER_QUERY')    \n",
    "if sample_size_per_query == None:\n",
    "    sample_size_per_query = input('Sample size per query: ') or sample_size_per_query_default\n",
    "    \n",
    "sample_size_per_query = int(sample_size_per_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 1  Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a YouTube API object\n",
    "youtube_o = youtube.api(\n",
    "    modules = modules,\n",
    "    headers = headers,\n",
    "    key = api_key,\n",
    "    experiment = api_experiment\n",
    ")\n",
    "\n",
    "# create a YouTube Internals API object\n",
    "youtubei_o = youtubei.api(\n",
    "    modules = modules,\n",
    "    headers = headers\n",
    ")\n",
    "\n",
    "pickle_proto = 3\n",
    "dataset = eda_utils.dataset(f'dsamples/youtube_search_{dataset_id}.dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_search_gen(*args, **kwargs):\n",
    "    from dcollect.utils.log import log\n",
    "    log.enable(level = log.levels.WARNING)\n",
    "    import concurrent.futures\n",
    "\n",
    "    df_search = None\n",
    "    df_info = None\n",
    "    df_channels = None\n",
    "    df_ads = None\n",
    "    \n",
    "    def worker_df_search(*args, **kwargs):\n",
    "        nonlocal df_search\n",
    "        df_search = df_from_json(\n",
    "            youtube_o.video.search(\n",
    "                *args, **kwargs\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def worker_df_info():\n",
    "        nonlocal df_info\n",
    "        df_info = df_from_json(\n",
    "            youtube_o.video.info(\n",
    "                id = df_search['id']\n",
    "            )\n",
    "        )\n",
    "            \n",
    "    def worker_df_ads():\n",
    "        nonlocal df_ads\n",
    "        df_ads = df_from_json(\n",
    "            youtubei_o.ad.placements(\n",
    "                id = df_search['id'],\n",
    "                throttle_size = 10\n",
    "            )\n",
    "        )\n",
    "            \n",
    "    def worker_df_channels():\n",
    "        nonlocal df_channels\n",
    "        df_channels = df_from_json(\n",
    "            youtube_o.channel.info(\n",
    "                id = df_search['creator.id']\n",
    "            )\n",
    "        )\n",
    "            \n",
    "    # - search\n",
    "    worker_df_search(*args, **kwargs)\n",
    "    \n",
    "    workers = [worker_df_info, worker_df_ads, worker_df_channels]\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = len(workers)) as executor:\n",
    "        for worker in workers:\n",
    "            executor.submit(worker)\n",
    "                \n",
    "    return df_search, df_info, df_channels, df_ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_search_gen_bulk(paramlist: list):\n",
    "    import concurrent.futures\n",
    "    \n",
    "    futures = []\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = len(paramlist)) as executor:\n",
    "        futures = [executor.submit(df_search_gen, **param) for param in paramlist]\n",
    "        \n",
    "    return [f.result() for f in futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "param_default = {\n",
    "    'count': sample_size_per_query\n",
    "}\n",
    "\n",
    "paramlist = []\n",
    "for c in string.ascii_lowercase:\n",
    "    param = dict(param_default)\n",
    "    param.update({\n",
    "        'keyword': c\n",
    "    })\n",
    "    paramlist.append(param)\n",
    "    \n",
    "df_search = pd.DataFrame()\n",
    "df_info = pd.DataFrame()\n",
    "df_channels = pd.DataFrame()\n",
    "df_ads = pd.DataFrame()\n",
    "\n",
    "results = df_search_gen_bulk(paramlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose(l):\n",
    "    return list(map(list, zip(*l)))\n",
    "\n",
    "df_search_res, df_info_res, df_channels_res, df_ads_res = transpose(results)\n",
    "\n",
    "df_search = pd.concat(df_search_res, copy = False)\n",
    "df_info = pd.concat(df_info_res, copy = False)\n",
    "df_channels = pd.concat(df_channels_res, copy = False)\n",
    "df_ads = pd.concat(df_ads_res, copy = False)\n",
    "\n",
    "dataset.update('youtube_search.pkl', df_search, overwrite = True, proto = pickle_proto)\n",
    "dataset.update('youtube_search_info.pkl', df_info, overwrite = True, proto = pickle_proto)\n",
    "dataset.update('youtube_search_ads.pkl', df_ads, overwrite = True, proto = pickle_proto)\n",
    "dataset.update('youtube_search_channels.pkl', df_channels, overwrite = True, proto = pickle_proto)\n",
    "\n",
    "df_report(df_search, name = 'Search Result (Original)')\n",
    "df_report(df_info, name = 'Info (Original)')\n",
    "df_report(df_channels, name = 'Channels (Original)')\n",
    "df_report(df_ads, name = 'Ad Placements (Original)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 2  Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - * (filter)\n",
    "def drop_common(df, df_other, *args, **kwargs):\n",
    "    return df.drop(columns = df.columns & df_other.columns, *args, **kwargs)\n",
    "\n",
    "# - search\n",
    "df_search.set_index(['id'], inplace = True)\n",
    "# - info\n",
    "df_info.set_index(['id'], inplace = True)\n",
    "# - channels\n",
    "df_channels = df_channels.add_prefix('creator.')\n",
    "df_channels.set_index(['creator.id'], inplace = True)\n",
    "# - ads\n",
    "df_ads.set_index(['id'], inplace = True)\n",
    "\n",
    "# drop common columns to avoid clashing\n",
    "# in this case, only `df_search` and `df_info` have merging conflicts\n",
    "drop_common(df_search, df_info, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - search (with details)\n",
    "df_search_details = df_search.copy()\n",
    "# - info\n",
    "df_search_details = df_search_details.merge(\n",
    "    df_info, \n",
    "    right_index = True, \n",
    "    left_on = 'id', \n",
    "    copy = False\n",
    ")\n",
    "# - ads\n",
    "df_search_details = df_search_details.merge(\n",
    "    df_ads, \n",
    "    right_index = True, \n",
    "    left_on = 'id', \n",
    "    copy = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 3  Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a brief look at our data\n",
    "df_report(df_search_details, name = 'Search Result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 4  Data Archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.update('youtube_search_details.pkl', df_search_details, proto = pickle_proto)\n",
    "# verify that we saved the correct data\n",
    "df_report(dataset.load('youtube_search_details.pkl'), name = 'Search Result (Verification)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
